üîπ Why These Design Choices Were Made (Key Decisions)

This section explains why specific tools and approaches were chosen, demonstrating a clear understanding of the system design.

1Ô∏è‚É£ Why Retrieval Augmented Generation (RAG) Was Used

Purpose:
To reduce hallucination and ensure answers are grounded in real data.

Reasoning:

LLMs alone generate answers based on learned patterns, not live or private data

RAG combines retrieval + generation, ensuring responses are based on retrieved documents

This makes the system more reliable, explainable, and production-ready

Outcome:

Answers are derived from actual document content

The system can be updated simply by changing stored documents

2Ô∏è‚É£ Why a Vector Database Was Needed

Purpose:
To enable semantic search rather than keyword matching.

Reasoning:

Keyword search fails when phrasing differs

Vector similarity captures meaning, not exact words

Semantic search is foundational for modern AI applications like RAG and recommendations

Outcome:

Queries retrieve relevant content even with different wording

Better recall and relevance than traditional search

3Ô∏è‚É£ Why Endee Was Chosen as the Vector Database

Purpose:
To use a high-performance, purpose-built vector database for semantic retrieval.

Reasoning:

Endee is designed specifically for dense vector search

It supports cosine similarity on high-dimensional embeddings

It aligns directly with RAG and semantic search use cases

Outcome:

Endee acts as the system‚Äôs semantic memory

Clear separation between ML logic and storage layer

4Ô∏è‚É£ Why Sentence Transformers Were Used for Embeddings

Purpose:
To convert raw text into meaningful dense vectors.

Reasoning:

Sentence Transformers produce semantically rich embeddings

all-MiniLM-L6-v2 is lightweight, fast, and widely used

Produces 384-dimensional vectors, compatible with Endee

Important Clarification:

Vectors are never manually created

All vectors are generated by a trained embedding model

Outcome:

Consistent semantic space for documents and queries

Reliable similarity comparisons

5Ô∏è‚É£ Why Text Chunking Was Used

Purpose:
To improve retrieval precision and relevance.

Reasoning:

Large documents dilute semantic focus

Chunking allows fine-grained retrieval

Smaller chunks improve alignment between query and content

Outcome:

More precise retrieval

Better answer quality in RAG

6Ô∏è‚É£ Why an Extractive QA Model Was Used

Purpose:
To ensure answers come strictly from retrieved context.

Reasoning:

Extractive QA models select answers directly from text

Prevents hallucinated or unsupported responses

Easier to verify correctness

Outcome:

High trustworthiness

Clear grounding in documents

7Ô∏è‚É£ Why a Lightweight Agentic Layer Was Added

Purpose:
To introduce decision-making without instability.

Reasoning:

Blind single-step retrieval can fail on weak matches

Evaluating similarity scores allows confidence checks

Controlled logic avoids infinite loops and unpredictable behavior

Outcome:

Improved reliability

Deterministic, explainable agent-like behavior

8Ô∏è‚É£ Why UI-Based Vector Insertion Was Used

Purpose:
To adapt to current tooling limitations while preserving correctness.

Reasoning:

Endee‚Äôs public ingestion APIs are currently limited

Vectors are still generated programmatically

UI insertion is a transport workaround, not a conceptual shortcut

Outcome:

Correct semantic pipeline

Honest and documented integration approach
