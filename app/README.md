# Application Design & Architecture

This document outlines how the RAG (Retrieval Augmented Generation) system works, the technologies we use, and the design decisions behind them.

## What is Retrieval Augmented Generation (RAG)?

Large Language Models (LLMs) are powerful, but they have some real limitations:

- They can make up answers (hallucination)
- They don't have access to your private or specialized data
- Their knowledge can't be updated on the fly

RAG solves these problems by finding relevant documents first, then using those documents to ground the model's response. Think of it as giving the model a reference book before asking a question.

This project uses RAG to ensure:
- Answers come from actual content you have
- You can see where the answer comes from
- Responses are based on meaning, not just keyword matching

---

## Why Endee as Our Vector Database?

**The Problem with Traditional Search**

Keyword-based search (like Ctrl+F) only finds exact matches. If you search for "canine," it won't find documents about "dogs" — even though they mean the same thing.

**Vector Databases to the Rescue**

Vector databases store documents as numbers (vectors) that represent their meaning. When you search, the system finds documents with similar meaning, not just matching words.

**Why Endee?**

- Fast, efficient search even with large amounts of data
- Built specifically for semantic (meaning-based) search
- Perfect fit for RAG pipelines

**What Endee Does:**
- Stores document embeddings (the numerical meaning representations)
- Performs similarity search (finding documents with similar meaning)
- Returns the most relevant chunks for your query

---

## How Do We Convert Text to Numbers? (Embedding Generation)

We use **sentence-transformers/all-MiniLM-L6-v2** to convert text into vectors.

**Why this model?**
- Produces high-quality semantic embeddings
- Lightweight and fast (won't slow down your system)
- Widely used in real production systems
- Creates 384-dimensional vectors (a format Endee understands)

**Important:** These vectors are always generated by a trained model. We never create them manually — that would lose the semantic meaning we worked hard to capture.

---

## How Documents Become Searchable (Ingestion Pipeline)

Documents go through a simple transformation:
Raw Text (or extracted PDF text)
        ↓
Text Chunking
        ↓
Embedding Generation
        ↓
Vector Storage in Endee


**Why split documents into chunks?**
- Smaller chunks = more precise search results
- Long documents can lose context, chunks keep it
- You can find specific sections, not just whole documents

**Current Implementation:**
- `ingest_pipeline.py` — Main ingestion logic
- `clipboard_ingest.py` — Helper for manual insertion via UI

---

## How Queries Get Answered (Retrieval & QA)

When you ask a question:

1. Your question gets converted to a vector (same way documents were)
2. Endee finds documents with similar meaning
3. The most relevant chunks are passed to a Question-Answering model
4. The model extracts the answer directly from those chunks

This approach prevents hallucination because the answer **must** come from the retrieved documents.

---

## Smart Refinement (Agentic RAG)

We added a simple decision-making layer that:
- Checks how confident the system is in the retrieved results
- If confidence is low, it refines the search and tries again
- All decisions are controlled and predictable (no wild autonomous behavior)

This helps avoid situations where the system gets stuck or makes unreliable decisions.

---

## Current Limitations & Trade-offs

**API Limitations**

Right now, Endee's APIs for vector insertion are limited, so we demonstrate the process through the UI. This is a tooling constraint, not a flaw in the RAG architecture. The semantic pipeline is fully correct and production-ready.

**PDF Handling**

PDFs are converted to text first (handled externally), then processed through the same pipeline as any other text. Once it's text, everything works the same.

---

## How This Mirrors Real Production Systems

This isn't a toy project — it reflects how real systems work:
- External services handle specialized tasks (like Endee)
- Clear boundaries between different components
- Automation is added gradually, not all at once
- Honest documentation of what we can and can't do yet

---

## What's Next?

We're planning to add:
- Automated API-based ingestion (no manual UI steps)
- Native PDF processing
- Support for multiple documents at once
- Recommendation features using the same vector database
- More advanced decision-making workflows

---

## Summary

This application demonstrates:
- ✓ A real RAG architecture (not over-simplified)
- ✓ Proper use of embeddings and vector search
- ✓ Thoughtful integration of specialized tools
- ✓ Honest about current constraints
- ✓ Clear reasoning behind every design choice
