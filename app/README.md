Application Design & Architecture

This document explains the internal design, reasoning, and concepts behind the RAG system implemented in this project.
It is intended to demonstrate a clear understanding of Retrieval Augmented Generation, vector databases, and system trade-offs.

1️⃣ Why Retrieval Augmented Generation (RAG)?

Large Language Models (LLMs) are powerful but have key limitations:

They may hallucinate answers

They lack access to private or domain-specific data

They cannot update knowledge dynamically

RAG addresses these limitations by grounding model responses in retrieved, relevant documents.

This project implements RAG to ensure:

Answers are based on real content

Responses are explainable

Semantic relevance drives generation

2️⃣ Role of Endee in the System

Endee is used as the vector database and serves as the system’s semantic memory.

Why a Vector Database?

Traditional keyword search fails to capture semantic meaning.
Vector databases enable similarity search based on meaning, not exact words.

Why Endee?

High-performance dense vector search

Designed for semantic retrieval use cases

Ideal backend for RAG pipelines

Endee is responsible for:

Storing document embeddings

Performing similarity search

Returning the most relevant chunks for a query

3️⃣ Embedding Generation (Why Sentence Transformers?)

Text is converted into vectors using:

sentence-transformers/all-MiniLM-L6-v2


Reasons for this choice:

Produces high-quality semantic embeddings

Lightweight and fast

Widely used in production RAG systems

384-dimensional vectors (compatible with Endee index)

Important:
Vectors are always generated by a trained embedding model — never manually created — to preserve semantic meaning.

4️⃣ Ingestion Pipeline (How Documents Become Searchable)

The ingestion pipeline follows this flow:

Raw Text (or extracted PDF text)
        ↓
Text Chunking
        ↓
Embedding Generation
        ↓
Vector Storage in Endee

Why Chunking?

Improves retrieval precision

Prevents loss of context due to long documents

Enables fine-grained semantic search

The ingestion logic is implemented in:

ingest_pipeline.py

clipboard_ingest.py (assisted insertion)

5️⃣ Retrieval Logic

For a given query:

The query is embedded using the same embedding model

Endee performs semantic similarity search

Top-K most relevant chunks are retrieved

This ensures semantic alignment between query and documents.

6️⃣ Agentic RAG Extension (Decision-Making Layer)

A lightweight agentic mechanism is implemented to improve reliability.

What makes this “agentic”?

Retrieved similarity scores are evaluated

If confidence is low, the system can refine retrieval

Execution is bounded and deterministic

This avoids:

Infinite loops

Unstable autonomous behavior

Prompt-only decision making

The goal is controlled intelligence, not blind autonomy.

7️⃣ Answer Generation (Grounded Output)

Retrieved text is passed to an extractive Question-Answering model.

Why extractive QA?

Prevents hallucination

Forces answers to come from retrieved context

Improves trustworthiness

This ensures:

If the answer exists, it must exist in the documents.

8️⃣ Design Trade-offs & Limitations
UI-Based Vector Insertion

At the time of implementation:

Endee’s public APIs are limited

Vector insertion and retrieval are demonstrated via UI

This is a tooling limitation, not a conceptual one.

The semantic pipeline remains:

Fully correct

Model-driven

RAG-compliant

PDF Handling

PDFs are treated as a preprocessing concern.
Once converted to text, the same ingestion pipeline applies.

9️⃣ Real-World Relevance

This architecture mirrors real production systems:

External vector databases

Service boundaries

Incremental automation

Explicit documentation of limitations

10️⃣ Future Extensions

Fully automated Endee ingestion via APIs

Native PDF ingestion

Multi-document RAG

Recommendation systems using the same vector store

Advanced agentic workflows with tool selection

Summary

This application demonstrates:

Correct RAG architecture

Proper use of vector embeddings

Thoughtful integration of Endee

Awareness of real-world constraints

Clear system-level reasoning
